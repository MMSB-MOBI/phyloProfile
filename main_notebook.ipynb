{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9eab6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f846d136",
   "metadata": {},
   "source": [
    "# Préparation de la matrice de profil phylogénétique\n",
    "\n",
    "La matrice de profil phylogénétique est réalisé en récupérant la liste des proteines, la\n",
    "liste des espèces bactériennes et l'information de présence ou d'absence d'une protéine\n",
    "chez une espèce donnée."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b26ff5b",
   "metadata": {},
   "source": [
    "# Binary phylogenetic matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b70c062",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('binary_phylogenetic_matrix.pkl'):\n",
    "    #build a dataframe of PA7 ortholog groups\n",
    "    file_path ='/Users/mdupuy/Documents/Stage/Pseudomonas_aeruginosa_PA7_119_ortholog_groups.csv'\n",
    "    df = pd.read_csv(file_path, delimiter=';')\n",
    "    #iterate through the dataframe to fill a dictionnary of phylogenetic profile\n",
    "    dico = {}\n",
    "    for row in df.itertuples():\n",
    "        protein = row[6] #Locus tag\n",
    "        id = row[5] #PGD Gene ID\n",
    "        file_path = f'/Users/mdupuy/Documents/Stage/All_COG_groups/COG_{id}.csv'\n",
    "        dataFrame = pd.read_csv(file_path)\n",
    "        strains = dataFrame['Strain'].to_list()\n",
    "        dico.setdefault(protein,[]).extend(strains)\n",
    "    #inverse the dictionnary\n",
    "    dicoinv={}\n",
    "    for keys, values in dico.items(): \n",
    "        for value in values: \n",
    "            dicoinv.setdefault(value,[]).append(keys)\n",
    "    #build a dataframe of binary phylogenetic profile, with protein id as index\n",
    "    # and bacterian species as columns \n",
    "    df = pd.DataFrame(data=0,index=dico.keys(),columns=dicoinv.keys())\n",
    "    for index in df.index:\n",
    "        for column in df.columns:\n",
    "            if column in dico[index]:\n",
    "                df.loc[index,column] = 1\n",
    "    save_dataframe('binary_phylogenetic_matrix.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84c099f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_phylo_matrix = pd.read_pickle('data/binary_phylogenetic_matrix.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292dcefb",
   "metadata": {},
   "source": [
    "Si besoin il est possible de \"débinariser\" la matrice en appliquant des poids grâce à la méthode suivante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56144cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_phylogenetic_matrix(phylogenetic_matrix):\n",
    "    #Return a weighted phylogenetic matrix from a binary phylogenetic matrix using inverse homology\n",
    "    g_size = len(df.index)\n",
    "    for column in df.columns:\n",
    "        homologue = df[column].to_numpy()\n",
    "        h_number = np.sum(homologue)\n",
    "        score = h_number/g_size\n",
    "        df[column].replace(1, score, inplace=True)\n",
    "        df.to_pickle('nonbinary_phylogenetic_matrix.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ffec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_phylo_matrix = weighted_phylogenetic_matrix(b_phylo_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f08a5d",
   "metadata": {},
   "source": [
    "# Score phylogenetic matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfb9eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('score_phylogenetic_matrix.pkl'):    \n",
    "    path = '/Users/mdupuy/Documents/Stage/Parser/Scores/'\n",
    "    dico_prot={}\n",
    "    for file in os.listdir(path):\n",
    "        dico_strain={}\n",
    "        df = pd.read_csv(f'{path}{file}', header=None)\n",
    "        protein = file.strip('_scores.txt')\n",
    "        dico_prot[protein]=dico_strain\n",
    "        for row in df.itertuples():\n",
    "            strain = row[3]\n",
    "            score = row[4]\n",
    "            dico_strain[strain]=score\n",
    "    df = pd.DataFrame(dico_prot)\n",
    "    df.to_pickle('score_phylogenetic_matrix.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f345d9",
   "metadata": {},
   "source": [
    "Une étape de prétraitement des matrice de score est nécessaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1936d4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def npp(df):\n",
    "    # remplace les valeurs manquante\n",
    "    df = df.fillna(0)\n",
    "    # corrige les valeurs à 0 pour prévenir les artefact\n",
    "    minimum = df[df > .01].min()\n",
    "    minimum = minimum.min()\n",
    "    df = df.where(df > minimum, minimum)\n",
    "    # normalisation par la taille\n",
    "    df = df.divide(df.max(axis=1),axis=0)\n",
    "    # transformation monotonique\n",
    "    df = 1/df\n",
    "    # z-score\n",
    "    df = (df - df.mean(axis=0))/df.std(ddof=0, axis=0)\n",
    "    #df = phylo_to_distance_matrix(df.T,2)\n",
    "    #df = df.corr()\n",
    "    #df = df.where(df>0,0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8fd2f2",
   "metadata": {},
   "source": [
    "Si la matrice utilisé est une matrice de score il est possible d'utiliser la décomposition en valeur singulière:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4111a904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd(df,threshold):\n",
    "    df = df.fillna(0)\n",
    "    # normalise chaque ligne par le score max de la ligne\n",
    "    df = df.divide(df.max(axis=1),axis=0)\n",
    "    print(\"first normalisation\")\n",
    "    # Apply the svd method to a score profile matrix to reduce it noise according to a\n",
    "    #certain threshold\n",
    "    u, s, vh = np.linalg.svd(df, full_matrices=False)\n",
    "    threshold = np.round_(len(s)*threshold//100)\n",
    "    print(threshold)\n",
    "    s[threshold:]=0\n",
    "    s = np.diag(s)\n",
    "    P = u.dot(s.dot(vh))\n",
    "    print(\"svd\")\n",
    "    # convertion des profil phylogénétique en vecteur unitaire\n",
    "    P_norm = np.linalg.norm(P,axis=1,keepdims=True)\n",
    "    P_u = P/P_norm\n",
    "    print(\"second normalisation\")\n",
    "    # Calacul de la corrélation de Pearson\n",
    "    df = pd.DataFrame(P_u,index=df.index,columns=df.columns)\n",
    "    #df = df.corr()\n",
    "    #print(\"correlation\")\n",
    "    #print(\"distance\")\n",
    "    #df = phylo_to_distance_matrix(df.T,2)\n",
    "    #df = df.where(df>0,0)\n",
    "    #print(\"treat negative\")\n",
    "    print(\"done\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a208cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_matrix = pd.read_pickle(\"score_phylogenetic_matrix.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6db6b57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "npp_phylo_matrix = npp(score_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9e3b38",
   "metadata": {},
   "source": [
    "# 1 - Réalisation de calcul de distance, corrélation et similarité\n",
    "\n",
    "A partir de notre matrice de profil phylogénétique nous pouvons calculer la distance, la corrélation ou la similarité entre deux protéines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "952a8194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix as dm\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea93808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phylo_to_distance_matrix(phylogenetic_matrix, p):\n",
    "    #Build the distance matrix with the Minkowski methode\n",
    "    df = phylogenetic_matrix\n",
    "    if p == 1:\n",
    "        methode = 'manhatthan' \n",
    "    elif p == 2:\n",
    "        methode = 'euclidean'\n",
    "    else:\n",
    "        methode = 'minkowski'\n",
    "    distance_matrix = dm(df, df, p)\n",
    "    distance_df = pd.DataFrame(\n",
    "    distance_matrix,\n",
    "    index = df.index,\n",
    "    columns = df.index\n",
    "    )\n",
    "    #distance_df = distance_df.div(distance_df.values.max())\n",
    "    return distance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9967583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming(phylogenetic_matrix_path):\n",
    "    df = pd.read_pickle(f'{phylogenetic_matrix_path}')\n",
    "    hamming = pdist(df, metric='hamming')\n",
    "    distance_matrix = squareform(hamming)\n",
    "    distance_df = pd.DataFrame(\n",
    "    distance_matrix,\n",
    "    index = df.index,\n",
    "    columns = df.index\n",
    "    )\n",
    "    #distance_df = distance_df.div(distance_df.values.max())\n",
    "    distance_df.to_pickle(f'hamming_distance_matrix.pkl')\n",
    "    return distance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f95b3ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = phylo_to_distance_matrix(score_matrix.T,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0734a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PSPA7_3376</th>\n",
       "      <th>PSPA7_2475</th>\n",
       "      <th>PSPA7_3669</th>\n",
       "      <th>PSPA7_6369</th>\n",
       "      <th>PSPA7_1922</th>\n",
       "      <th>PSPA7_2608</th>\n",
       "      <th>PSPA7_3414</th>\n",
       "      <th>PSPA7_6114</th>\n",
       "      <th>PSPA7_0943</th>\n",
       "      <th>PSPA7_2317</th>\n",
       "      <th>...</th>\n",
       "      <th>PSPA7_5214</th>\n",
       "      <th>PSPA7_1508</th>\n",
       "      <th>PSPA7_1986</th>\n",
       "      <th>PSPA7_0714</th>\n",
       "      <th>PSPA7_4275</th>\n",
       "      <th>PSPA7_5069</th>\n",
       "      <th>PSPA7_1775</th>\n",
       "      <th>PSPA7_0569</th>\n",
       "      <th>PSPA7_5576</th>\n",
       "      <th>PSPA7_0076</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PSPA7_3376</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSPA7_2475</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSPA7_3669</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSPA7_6369</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSPA7_1922</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56505.033513</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSPA7_5069</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSPA7_1775</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSPA7_0569</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSPA7_5576</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSPA7_0076</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6286 rows × 6286 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            PSPA7_3376  PSPA7_2475  PSPA7_3669  PSPA7_6369  PSPA7_1922  \\\n",
       "PSPA7_3376         NaN         NaN         NaN         NaN         NaN   \n",
       "PSPA7_2475         NaN         NaN         NaN         NaN         NaN   \n",
       "PSPA7_3669         NaN         NaN         NaN         NaN         NaN   \n",
       "PSPA7_6369         NaN         NaN         NaN         NaN         NaN   \n",
       "PSPA7_1922         NaN         NaN         NaN         NaN         0.0   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "PSPA7_5069         NaN         NaN         NaN         NaN         NaN   \n",
       "PSPA7_1775         NaN         NaN         NaN         NaN         NaN   \n",
       "PSPA7_0569         NaN         NaN         NaN         NaN         NaN   \n",
       "PSPA7_5576         NaN         NaN         NaN         NaN         NaN   \n",
       "PSPA7_0076         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "            PSPA7_2608  PSPA7_3414    PSPA7_6114  PSPA7_0943  PSPA7_2317  ...  \\\n",
       "PSPA7_3376         NaN         NaN           NaN         NaN         NaN  ...   \n",
       "PSPA7_2475         NaN         NaN           NaN         NaN         NaN  ...   \n",
       "PSPA7_3669         NaN         NaN           NaN         NaN         NaN  ...   \n",
       "PSPA7_6369         NaN         NaN           NaN         NaN         NaN  ...   \n",
       "PSPA7_1922         NaN         NaN  56505.033513         NaN         NaN  ...   \n",
       "...                ...         ...           ...         ...         ...  ...   \n",
       "PSPA7_5069         NaN         NaN           NaN         NaN         NaN  ...   \n",
       "PSPA7_1775         NaN         NaN           NaN         NaN         NaN  ...   \n",
       "PSPA7_0569         NaN         NaN           NaN         NaN         NaN  ...   \n",
       "PSPA7_5576         NaN         NaN           NaN         NaN         NaN  ...   \n",
       "PSPA7_0076         NaN         NaN           NaN         NaN         NaN  ...   \n",
       "\n",
       "            PSPA7_5214  PSPA7_1508  PSPA7_1986  PSPA7_0714  PSPA7_4275  \\\n",
       "PSPA7_3376         NaN         NaN         NaN         NaN         NaN   \n",
       "PSPA7_2475         NaN         NaN         NaN         NaN         NaN   \n",
       "PSPA7_3669         NaN         NaN         NaN         NaN         NaN   \n",
       "PSPA7_6369         NaN         NaN         NaN         NaN         NaN   \n",
       "PSPA7_1922         NaN         NaN         NaN         NaN         NaN   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "PSPA7_5069         NaN         NaN         NaN         NaN         NaN   \n",
       "PSPA7_1775         NaN         NaN         NaN         NaN         NaN   \n",
       "PSPA7_0569         NaN         NaN         NaN         NaN         NaN   \n",
       "PSPA7_5576         NaN         NaN         NaN         NaN         NaN   \n",
       "PSPA7_0076         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "            PSPA7_5069  PSPA7_1775  PSPA7_0569  PSPA7_5576  PSPA7_0076  \n",
       "PSPA7_3376         NaN         NaN         NaN         NaN         NaN  \n",
       "PSPA7_2475         NaN         NaN         NaN         NaN         NaN  \n",
       "PSPA7_3669         NaN         NaN         NaN         NaN         NaN  \n",
       "PSPA7_6369         NaN         NaN         NaN         NaN         NaN  \n",
       "PSPA7_1922         NaN         NaN         NaN         NaN         NaN  \n",
       "...                ...         ...         ...         ...         ...  \n",
       "PSPA7_5069         NaN         NaN         NaN         NaN         NaN  \n",
       "PSPA7_1775         NaN         NaN         NaN         NaN         NaN  \n",
       "PSPA7_0569         NaN         NaN         NaN         NaN         NaN  \n",
       "PSPA7_5576         NaN         NaN         NaN         NaN         NaN  \n",
       "PSPA7_0076         NaN         NaN         NaN         NaN         NaN  \n",
       "\n",
       "[6286 rows x 6286 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335389e5",
   "metadata": {},
   "source": [
    "# 2 - Définition de la matrice d'intéraction à partir d'un seuil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87622856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjency(df, threshold):\n",
    "    # Should I add a feature to determine if condition should be greater or lower\n",
    "    #depending on the type of data ?\n",
    "    df = df.applymap(lambda x: 1 if x<=threshold else 0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b6a31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = pd.read_pickle('hamming_distance_matrix.pkl')\n",
    "predicted_adjency = adjency(matrix, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec6c3a3",
   "metadata": {},
   "source": [
    "# 3 - Comparaison de la matrice d'intéraction avec Kegg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296ecdcf",
   "metadata": {},
   "source": [
    "Premièrement nous devons récupérer la liste des protéines présentent pour chaque pathways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d878db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bioservices import KEGG\n",
    "k = KEGG(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42c7086",
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_pathway = []\n",
    "for path in pathway_list:\n",
    "    res = k.get(f\"path:{path}\")\n",
    "    d = k.parse(res)\n",
    "    with open(f\"{path}.txt\",\"w\") as file:\n",
    "        if 'GENE' in d.keys():\n",
    "            for gene in d['GENE']:\n",
    "                file.write(f'{gene}\\n')\n",
    "        else:\n",
    "            file.write('gene not found')\n",
    "            problematic_pathway.append(path)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50af40e9",
   "metadata": {},
   "source": [
    "# Test de précision\n",
    "\n",
    "Obtenir la liste de toutes les protéines contenues dans tous les pathways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f709af",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/mdupuy/Documents/my_project/Pathways'\n",
    "dico = {}\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    pathway = file.strip('.txt')\n",
    "    gene = []\n",
    "    with open (f'{path}/{file}') as file:\n",
    "        for line in file:\n",
    "            if 'PSPA7' in line:\n",
    "                gene.append(line.strip('\\n'))\n",
    "    dico.setdefault(pathway,[]).extend(gene)\n",
    "\n",
    "#observed_adjency = pd.DataFrame(data=0,index=gene,columns=gene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ffa93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicoinv={}\n",
    "for keys, values in dico.items(): \n",
    "    for value in values: \n",
    "        dicoinv.setdefault(value,[]).append(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda3aa0e",
   "metadata": {},
   "source": [
    "Former une matrice d'adjacence observée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951e589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene = set(dicoinv.keys())\n",
    "observed_adjency = pd.DataFrame(data=0,index=gene,columns=gene)\n",
    "for index in observed_adjency.index:\n",
    "        for column in observed_adjency.columns:\n",
    "            if not set(dicoinv[index]).isdisjoint(set(dicoinv[column])):\n",
    "                observed_adjency.loc[index,column] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd8187b",
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_adjency.to_pickle('data/observed_adjency_matrix.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa6867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_intersect(df):\n",
    "    print('intersecting')\n",
    "    distance_matrix = df\n",
    "    observed_adjency = pd.read_pickle('data/observed_adjency_matrix.pkl')\n",
    "\n",
    "    d_ind = distance_matrix.index\n",
    "\n",
    "    o_ind = observed_adjency.index\n",
    "\n",
    "    ind = d_ind.intersection(o_ind)\n",
    "\n",
    "    distance_matrix = distance_matrix.reindex(index=ind, columns=ind)\n",
    "\n",
    "    dico = distance_matrix.to_dict('split')\n",
    "    print('intersected')\n",
    "    return dico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d523d595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_couples(index_list):\n",
    "    for i in range(len(index_list)):\n",
    "        for j in range(i+1,len(index_list)):\n",
    "            yield((index_list[i],index_list[j]))\n",
    "\n",
    "def couple_sorter(df):\n",
    "    print('sorting')\n",
    "    couple_dist={}\n",
    "    dico = df.to_dict('split')\n",
    "    for couple in get_couples(dico['index']):\n",
    "        dist=df.loc[couple]\n",
    "        couple_dist.setdefault(dist,[]).append(couple)\n",
    "    couple_dist_ord = dict(sorted(couple_dist.items(),reverse=False))\n",
    "    #print(couple_dist_ord)\n",
    "    print('sorted')\n",
    "    return couple_dist_ord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40ec29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def courbe_rc(dico,shuffle):\n",
    "    print('ploting')\n",
    "    observed_adjency = pd.read_pickle('data/observed_adjency_matrix.pkl')\n",
    "    P_list = []\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    n = 1\n",
    "    value_list = list(dico.values())\n",
    "    if shuffle == True:\n",
    "        random.shuffle(value_list)\n",
    "    for value in value_list:\n",
    "        for couple in value:\n",
    "            if (observed_adjency.loc[couple[0],couple[1]]) == 1:\n",
    "                TP = TP+1\n",
    "            else:\n",
    "                FP = FP+1\n",
    "            n=n+1\n",
    "            if n==1000:\n",
    "                Precision = TP/(TP+FP)\n",
    "                P_list.append(Precision)\n",
    "                n = 1\n",
    "    print('ploted')\n",
    "    return P_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186a38a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(list_path):\n",
    "    dico_rc = {}\n",
    "    for path in list_path:\n",
    "        distance_matrix = pd.read_pickle(path)\n",
    "        intersect_matrix = df_intersect(distance_matrix)\n",
    "        couple_dist = couple_sorter(intersect_matrix)\n",
    "        rc_ord = courbe_rc(couple_dist, False)\n",
    "        #rc_rand = courbe_rc(dico_dist, True)\n",
    "        dico_rc[path] = rc_ord\n",
    "    rc_rand = courbe_rc(couple_dist, True)\n",
    "    dico_rc['rand'] = rc_rand\n",
    "    return dico_rc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516dfb63",
   "metadata": {},
   "source": [
    "Effectuer test de comparaison entre matrice d'adjacence prédite et observée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acee7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_adjency = pd.read_pickle('data/observed_adjency_matrix.pkl')\n",
    "\n",
    "quantiles = [quantile/10 for quantile in range(0,11,1)]\n",
    "quantiles = np.quantile(distance_matrix, quantiles)\n",
    "print(quantiles)\n",
    "\n",
    "dico_rc = {}\n",
    "dico_ROC = {}\n",
    "#build the predicted adjency matrix\n",
    "for threshold in quantiles:\n",
    "    #predicted the adjency matrix from the distance matrix for a specific threshold\n",
    "    predicted_adjency = adjency(distance_matrix, threshold)\n",
    "    predicted_adjency = predicted_adjency.reindex_like(observed_adjency)\n",
    "    #print(predicted_adjency)\n",
    "    #gathered the upper triangular matrix of the two adjency matrix\n",
    "    observed = observed_adjency.to_numpy()\n",
    "    predicted = predicted_adjency.to_numpy()\n",
    "    observed = observed[np.triu_indices_from(observed,1)]\n",
    "    predicted = predicted[np.triu_indices_from(predicted,1)]\n",
    "    #build the confusion matrix\n",
    "    confusion_matrix = pd.crosstab(predicted,observed,rownames=['predicted'], colnames=['observed'])\n",
    "    print(confusion_matrix)\n",
    "    #calculate recall and precision and stock them in a dictionnary\n",
    "    '''    \n",
    "    TP = confusion_matrix.loc[1,1] \n",
    "    FP = confusion_matrix.loc[1,0]\n",
    "    TN = confusion_matrix.loc[0,0]\n",
    "    FN = confusion_matrix.loc[0,1]\n",
    "    try:\n",
    "        TP = confusion_matrix.loc[1,1] \n",
    "        FP = confusion_matrix.loc[1,0]\n",
    "        TN = confusion_matrix.loc[0,0]\n",
    "        FN = confusion_matrix.loc[0,1]\n",
    "        print(f'TP:{TP}, FP:{FP}, TN:{TN},FN:{FN}')\n",
    "    '''\n",
    "    if not 0 in confusion_matrix.columns:\n",
    "        TN = 0\n",
    "        FN = 0\n",
    "        print('a')\n",
    "    if not 1 in confusion_matrix.columns:\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        print('b')\n",
    "    if not 0 in confusion_matrix.index:\n",
    "        TN = 0\n",
    "        FP = 0\n",
    "        print('c')\n",
    "    if not 1 in confusion_matrix.index:\n",
    "        TP = 0\n",
    "        FN = 0\n",
    "        print('d')\n",
    "    else:\n",
    "        TP = confusion_matrix.loc[1,1] \n",
    "        FP = confusion_matrix.loc[1,0]\n",
    "        TN = confusion_matrix.loc[0,0]\n",
    "        FN = confusion_matrix.loc[0,1]\n",
    "        print(f'TP:{TP}, FP:{FP}, TN:{TN},FN:{FN}')\n",
    "    try:    \n",
    "        Recall = TP/(TP+FN)\n",
    "        Precision = TP/(TP+FP)\n",
    "        TPR = TP/(TP+FN)\n",
    "        FPR = FP/(FP+TN)\n",
    "    except:\n",
    "        continue\n",
    "    dico_rc[threshold] = (Recall,Precision)\n",
    "    dico_ROC[threshold] = (FPR,TPR)\n",
    "    print(f'threshold: {threshold} R & P: {dico_rc[threshold]}')\n",
    "    print(f'threshold: {threshold} FPR & TPR: {dico_ROC[threshold]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989403fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_roc = {} #key threshold, value (FPR,TPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf52c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#return predicted adjency matrix according to a threshold and organized like the obseved adjency matrix\n",
    "matrix = pd.read_pickle('hamming_distance_matrix.pkl')\n",
    "predicted_adjency = adjency(matrix, 0)\n",
    "predicted_adjency = predicted_adjency.reindex_like(observed_adjency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69158df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_matrix = predicted_adjency.reindex_like(observed_adjency)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
