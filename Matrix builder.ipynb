{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "836fa0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea762f2",
   "metadata": {},
   "source": [
    "# Binary matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07e24f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This script is able to build a coocurence matrix from a list of COG of an organism.\n",
    "It needs to take a csv file containing the list of the COG and the directory where the\n",
    "COG files are stored.\n",
    "It returns a numpy array.\n",
    "'''\n",
    "def iter_query_cog(query_orth):\n",
    "    with open(query_orth) as csvfile:\n",
    "        next(csvfile)\n",
    "        for l in csvfile:\n",
    "            buffer = l.split(';')\n",
    "            yield([buffer[4],buffer[5]])\n",
    "\n",
    "def cog_2_g_list(cog_id,cog_dir):\n",
    "    file_path = f'{cog_dir}/COG_{cog_id}.csv'\n",
    "    g_list = list()\n",
    "    with open(file_path) as csvfile:\n",
    "        next(csvfile)\n",
    "        for l in csvfile:\n",
    "            buffer = l.split(',')\n",
    "            strain = buffer[1]\n",
    "            g_list.append(strain)\n",
    "    return g_list\n",
    "\n",
    "def p_dict_builder(query_orth,cog_dir):\n",
    "    p_dict={}\n",
    "    i=0\n",
    "    for cog_id,p_name in iter_query_cog(query_orth):\n",
    "        g_list = cog_2_g_list(cog_id,cog_dir)\n",
    "        p_dict[p_name] = g_list\n",
    "        #i=i+1\n",
    "        #if i>9:\n",
    "            #break\n",
    "    return p_dict\n",
    "\n",
    "def index_setter(p_dict):\n",
    "    p_index = list()\n",
    "    g_index = set()\n",
    "    for p_name,g_list in p_dict.items():\n",
    "        p_index.append(p_name)\n",
    "        g_index.update(g_list)\n",
    "    g_index = list(g_index)\n",
    "    return p_index,g_index\n",
    "\n",
    "def set_M_values(M,m_index,g_name,p_name,value): \n",
    "    i = m_index[0].index(p_name) #protein index\n",
    "    j = m_index[1].index(g_name) #genome index\n",
    "    M[i,j] = value\n",
    "\n",
    "#main function\n",
    "def matrix_builder(query_orth,cog_dir,value):\n",
    "    #generate a dictionnary of protein coocurences\n",
    "    p_dict=p_dict_builder(query_orth,cog_dir)\n",
    "    with open('p_dict.pkl', 'wb') as file:\n",
    "        pickle.dump(p_dict, file)\n",
    "    #generate the coocurences matrix index\n",
    "    m_index=index_setter(p_dict)\n",
    "    with open('m_index.pkl', 'wb') as file:\n",
    "        pickle.dump(m_index, file)\n",
    "    #generate the coocurences matrix\n",
    "    x = len(m_index[0])\n",
    "    y = len(m_index[1])\n",
    "    M = np.zeros((x,y))\n",
    "    for p_name,g_list in p_dict.items():\n",
    "        for g_name in g_list:\n",
    "            set_M_values(M,m_index,g_name,p_name,value)\n",
    "    with open('p_matrix.npy', 'wb') as f:\n",
    "        np.save(f, M)\n",
    "    return M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8592ba29",
   "metadata": {},
   "source": [
    "To build a binary matrix use the matrix builder function.\n",
    "Feed it with a csv file listing the ortholog associated to your strain query \"query_orth\"\n",
    "and a directory containing all the COG groups \"cog_dir\".\n",
    "Set the value to \"1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70acca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query_orth ='/Users/mdupuy/Documents/Stage/Pseudomonas_aeruginosa_PA7_119_ortholog_groups.csv'\n",
    "#query_orth = 'Data/COG/Pseudomonas_aeruginosa_PA7_119_ortholog_groups.csv'\n",
    "cog_dir ='/Users/mdupuy/Documents/Stage/All_COG_groups/'\n",
    "#cog_dir='Data/COG/All_COG_groups/'\n",
    "binary_matrix = matrix_builder(query_orth,cog_dir,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06114b8b",
   "metadata": {},
   "source": [
    "# Score Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e04e001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "'''\n",
    "This script is able to build a coocurence matrix from a list of protein alignment score\n",
    "from sevaral organism against a query one.\n",
    "It needs to take txt files containing alignment score information.\n",
    "It returns a numpy array.\n",
    "'''\n",
    "def score_p_dict_builder(score_file_dir):\n",
    "    path = score_file_dir\n",
    "    dico_prot={}\n",
    "    for file in os.listdir(path):\n",
    "        if re.search('.+scores.txt', file) : #add security to iterate only over the score file\n",
    "            dico_strain={}\n",
    "            protein = file.strip('_scores.txt')\n",
    "            file_path = f'{path}{file}'\n",
    "            with open(file_path,'r') as file:\n",
    "                for l in file:\n",
    "                    buffer = l.split(',')\n",
    "                    strain = buffer[2]\n",
    "                    score = buffer[3].strip('\\n')\n",
    "                    dico_strain[strain]=score\n",
    "            dico_prot[protein]=dico_strain\n",
    "    return(dico_prot)        \n",
    "\n",
    "def score_matrix_builder(score_file_dir,title):\n",
    "    if not os.path.isfile(f'p_dict_{title}.pkl'):\n",
    "        p_dict = score_p_dict_builder(score_file_dir)\n",
    "        with open(f'p_dict_{title}.pkl', 'wb') as file:\n",
    "            pickle.dump(p_dict, file)\n",
    "    else:\n",
    "        with open(f'p_dict_{title}.pkl', 'rb') as file:\n",
    "            p_dict = pickle.load(file)\n",
    "    if not os.path.isfile(f'm_index_{title}.pkl'):\n",
    "        m_index = index_setter(p_dict)\n",
    "        with open(f'm_index_{title}.pkl', 'wb') as file:\n",
    "            pickle.dump(m_index, file)\n",
    "    else:\n",
    "        with open(f'm_index_{title}.pkl', 'rb') as file:\n",
    "            m_index = pickle.load(file)\n",
    "    x = len(m_index[0])\n",
    "    y = len(m_index[1])\n",
    "    M = np.zeros((x,y))\n",
    "    for p_name,g_list in p_dict.items():\n",
    "            for g_name in g_list:\n",
    "                i = m_index[0].index(p_name) #protein index\n",
    "                j = m_index[1].index(g_name) #genome index\n",
    "                M[i,j] = p_dict[p_name][g_name]\n",
    "    #if not os.path.isfile(f'p_matrix_{title}.npy'):\n",
    "        #with open(f'p_matrix_{title}.npy', 'wb') as f:\n",
    "            #np.save(f, M)\n",
    "    #else:\n",
    "        #with open(f'p_matrix_{title}.npy', 'rb') as f:\n",
    "            #M = np.load(f,allow_pickle=True)\n",
    "    return(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cec48ab",
   "metadata": {},
   "source": [
    "To build a score matrix use the score_matrix_builder function.\n",
    "Feed it with directory containing all the alligment socre data \"score_file_dire\".\n",
    "Define a title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daa20854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 1s, sys: 1.28 s, total: 3min 2s\n",
      "Wall time: 3min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "score_file_dir = 'Data/Score/'\n",
    "score_matrix = score_matrix_builder(score_file_dir,'score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdf759f",
   "metadata": {},
   "source": [
    "# KEGG Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7007472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This script is able to build a pathway*protein matrix from a list of pathway.\n",
    "It needs to take txt files containing pathway information.\n",
    "It returns a numpy array.\n",
    "'''\n",
    "from bioservices import KEGG\n",
    "\n",
    "def iter_pathway(list_pathway):\n",
    "    with open(list_pathway) as file:\n",
    "        for l in file:\n",
    "            if l.strip():\n",
    "                buffer = l.split()\n",
    "                yield(buffer[0])\n",
    "                \n",
    "def KEGG_request(list_pathway, path):\n",
    "    # use a list of pathway to request all the gene associated with each pathway\n",
    "    k = KEGG(verbose=False)\n",
    "    problematic_pathway = []\n",
    "    list_pathway = f'{path}{list_pathway}'\n",
    "    for pathway in iter_pathway(path_list_pathway):\n",
    "        res = k.get(f\"path:{pathway}\")\n",
    "        d = k.parse(res)\n",
    "        with open(f\"{path}{pathway}.txt\",\"w\") as file:\n",
    "            if 'GENE' in d.keys():\n",
    "                for gene in d['GENE']:\n",
    "                    file.write(f'{gene}\\n')\n",
    "            else:\n",
    "                file.write('gene not found')\n",
    "                problematic_pathway.append(path)\n",
    "                continue\n",
    "            \n",
    "def pathway_dict_builder(list_pathway,path):\n",
    "    dico = {}\n",
    "    list_pathway = f'{path}{list_pathway}'\n",
    "    for pathway in iter_pathway(list_pathway):\n",
    "        file_path = f'{path}{pathway}.txt'\n",
    "        protein = []\n",
    "        with open(file_path) as file:\n",
    "            for l in file:\n",
    "                protein.append(l.strip('\\n'))\n",
    "        dico.setdefault(pathway,[]).extend(protein)  \n",
    "    return(dico)\n",
    "\n",
    "def kegg_matrix_builder(list_pathway,path):\n",
    "    pathway_dict = pathway_dict_builder(list_pathway,path)\n",
    "    m_index = index_setter(pathway_dict)\n",
    "    x = len(m_index[0])\n",
    "    y = len(m_index[1])\n",
    "    M = np.zeros((x,y))\n",
    "    for p_name,g_list in pathway_dict.items():\n",
    "        for g_name in g_list:\n",
    "            i = m_index[0].index(p_name) #protein index\n",
    "            j = m_index[1].index(g_name) #genome index\n",
    "            M[i,j] = 1\n",
    "    return M\n",
    "\n",
    "def kegg_adjency(path):\n",
    "    dico = {}\n",
    "    for file in os.listdir(path):\n",
    "        if re.search('pap.+.txt', file) : #add security to iterate only over the score file\n",
    "            pathway = file.strip('.txt')\n",
    "            gene = []\n",
    "            with open (f'{path}/{file}') as file:\n",
    "                for line in file:\n",
    "                    if 'PSPA7' in line:\n",
    "                        gene.append(line.strip('\\n'))\n",
    "            dico.setdefault(pathway,[]).extend(gene)  \n",
    "    dicoinv={}\n",
    "    for keys, values in dico.items(): \n",
    "        for value in values: \n",
    "            dicoinv.setdefault(value,[]).append(keys)      \n",
    "    gene = set(dicoinv.keys())\n",
    "    observed_adjency = pd.DataFrame(data=0,index=gene,columns=gene)\n",
    "    for index in observed_adjency.index:\n",
    "            for column in observed_adjency.columns:\n",
    "                if not set(dicoinv[index]).isdisjoint(set(dicoinv[column])):\n",
    "                    observed_adjency.loc[index,column] = 1               \n",
    "    return observed_adjency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fac2cf8",
   "metadata": {},
   "source": [
    "To build a KEGG Matrix use the kegg_matrix_builder function. Feed it with a directory containing file describing pathway and the list of pathway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a03b01",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path = 'Data/KEGG/'\n",
    "list_pathway = 'List_126_KEGG_Path_PA7.txt'\n",
    "#get every gene related to each pathway\n",
    "KEGG_request(list_pathway,path)\n",
    "#draw a matrix of gene*pathway\n",
    "kegg_matrix = kegg_matrix_builder(list_pathway,path)\n",
    "#draw an adjency matrix from KEGG pathway\n",
    "observed_adjency = kegg_adjency(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a997020",
   "metadata": {},
   "source": [
    "#  Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bf41810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def svd(M,threshold):\n",
    "    '''\n",
    "    Apply the svd method to a score profile matrix to reduce it noise according to a\n",
    "    certain threshold\n",
    "    '''\n",
    "    M_xmax = np.amax(M,axis=1)\n",
    "    M_lnorm = np.divide(M.T,M_xmax).T\n",
    "    print(\"first normalisation\")\n",
    "    u, s, vh = np.linalg.svd(M_lnorm, full_matrices=False)\n",
    "    s[threshold:]=0.0\n",
    "    P = np.dot(u * s, vh)\n",
    "    print(\"svd\")\n",
    "    #P_norm = np.linalg.norm(P,keepdims=True,axis=0) #normalize the columns\n",
    "    P_norm = np.linalg.norm(P,keepdims=True,axis=1) #normalize the rows\n",
    "    P_u = np.divide(P,P_norm)\n",
    "    print(\"second normalisation\")\n",
    "    print(\"done\")\n",
    "    return P_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c51146e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def npp(M,transformation=True):\n",
    "    '''\n",
    "    Apply the npp methode to a score profile matrix\n",
    "    '''\n",
    "    M=score_matrix\n",
    "    # corrige les valeurs à 0 pour prévenir les artefact\n",
    "    minval = np.min(M[np.nonzero(M)])\n",
    "    score_matrix[score_matrix==0]=minval\n",
    "    # normalisation par la taille\n",
    "    M_xmax = np.amax(M,axis=1)\n",
    "    M_lnorm = np.divide(M.T,M_xmax).T\n",
    "    # transformation monotonique\n",
    "    if transformation == True:\n",
    "        M_transf = np.reciprocal(M_lnorm)\n",
    "    else:\n",
    "        M_transf = M_lnorm\n",
    "    # z-score\n",
    "    M_ymean = np.mean(M_transf,axis=0,keepdims=True)\n",
    "    M_ystd = np.std(M_transf,ddof=0,axis=0,keepdims=True)\n",
    "    M = np.divide(np.subtract(M,M_ymean),M_ystd)\n",
    "    return M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c5ed57",
   "metadata": {},
   "source": [
    "# Distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25a59877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "from scipy.spatial.distance import squareform\n",
    "def distance_matrix_builder(coocurencies_matrix,metric):\n",
    "    '''\n",
    "    the metric use to calculate the distance can be:\n",
    "    - 'hamming' for binary matrix, \n",
    "    - 'euclidean' and 'correlation' for score matrix\n",
    "    '''\n",
    "    if not os.path.isfile(f'{metric}_distance_matrix.pkl'):\n",
    "        distance = pdist(coocurencies_matrix, metric=metric)\n",
    "        distance_matrix = squareform(distance)\n",
    "        with open(f'{metric}_distance_matrix.pkl', 'wb') as f:\n",
    "            pickle.dump(distance_matrix,f)\n",
    "    else:\n",
    "        with open(f'{metric}_distance_matrix.pkl', 'rb') as file:\n",
    "            distance_matrix = pickle.load(file)\n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b20542c",
   "metadata": {},
   "source": [
    "To calculate the distance between pairwise protein use distance_matrix_builder. Feed it with coocurencies_matrix\n",
    "and use a metric such as 'hamming', 'euclidean' or 'correlation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6a2e2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#binary_distance_matrix = distance_matrix_builder(binary_matrix,'hamming')\n",
    "#score_distance_matrix = distance_matrix_builder(score_matrix,'euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62aeda6",
   "metadata": {},
   "source": [
    "# Test de précision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56af1ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "def df_intersect(df):\n",
    "    print('intersecting')\n",
    "    distance_matrix = df\n",
    "    observed_adjency = pd.read_pickle('Data/Result/pkl/observed_adjency_matrix.pkl')\n",
    "\n",
    "    d_ind = distance_matrix.index\n",
    "\n",
    "    o_ind = observed_adjency.index\n",
    "\n",
    "    ind = d_ind.intersection(o_ind)\n",
    "\n",
    "    distance_matrix = distance_matrix.reindex(index=ind, columns=ind)\n",
    "\n",
    "    #dico = distance_matrix.to_dict('split')\n",
    "    print('intersected')\n",
    "    return distance_matrix\n",
    "\n",
    "# can be replace by itertool.combinations(index_list,2)\n",
    "def get_couples(index_list):\n",
    "    for i in range(len(index_list)):\n",
    "        for j in range(i+1,len(index_list)):\n",
    "            yield((index_list[i],index_list[j]))\n",
    "\n",
    "def couple_sorter(df):\n",
    "    print('sorting')\n",
    "    couple_dist={}\n",
    "    dico = df.to_dict('split')\n",
    "    for couple in get_couples(dico['index']):\n",
    "        dist=df.loc[couple]\n",
    "        couple_dist.setdefault(dist,[]).append(couple)\n",
    "    couple_dist_ord = dict(sorted(couple_dist.items(),reverse=False))\n",
    "    #print(couple_dist_ord)\n",
    "    print('sorted')\n",
    "    return couple_dist_ord\n",
    "\n",
    "def courbe_rc(dico,shuffle):\n",
    "    print('ploting')\n",
    "    observed_adjency = pd.read_pickle('Data/Result/pkl/observed_adjency_matrix.pkl')\n",
    "    P_list = []\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    n = 1\n",
    "    value_list = list(dico.values())\n",
    "    if shuffle == True:\n",
    "        random.shuffle(value_list)\n",
    "    for value in value_list:\n",
    "        for couple in value:\n",
    "            if (observed_adjency.loc[couple[0],couple[1]]) == 1:\n",
    "                TP = TP+1\n",
    "            else:\n",
    "                FP = FP+1\n",
    "            n=n+1\n",
    "            if n==1000:\n",
    "                Precision = TP/(TP+FP)\n",
    "                P_list.append(Precision)\n",
    "                n = 1\n",
    "    print('ploted')\n",
    "    return P_list\n",
    "\n",
    "def benchmark(list_path):\n",
    "    dico_rc = {}\n",
    "    for path in list_path:\n",
    "        distance_matrix = pd.read_pickle(path)\n",
    "        intersect_matrix = df_intersect(distance_matrix)\n",
    "        couple_dist = couple_sorter(intersect_matrix)\n",
    "        rc_ord = courbe_rc(couple_dist, False)\n",
    "        dico_rc[path] = rc_ord\n",
    "    rc_rand = courbe_rc(couple_dist, True)\n",
    "    dico_rc['rand'] = rc_rand\n",
    "    return dico_rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5c07a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#1 - build the score matrix\n",
    "score_file_dir = 'Data/Score/'\n",
    "score_matrix = score_matrix_builder(score_file_dir,'score')\n",
    "\n",
    "#2 - apply the svd methode\n",
    "svd_matrix = svd(score_matrix,10)\n",
    "\n",
    "#3 - calculate the distance\n",
    "score_distance_matrix = distance_matrix_builder(M,'euclidean')\n",
    "\n",
    "#4 - build the golden standard\n",
    "path = 'Data/KEGG/'\n",
    "list_pathway = 'List_126_KEGG_Path_PA7.txt'\n",
    "#get every gene related to each pathway\n",
    "KEGG_request(list_pathway,path)\n",
    "#draw a matrix of gene*pathway\n",
    "kegg_matrix = kegg_matrix_builder(list_pathway,path)\n",
    "#draw an adjency matrix from KEGG pathway\n",
    "observed_adjency = kegg_adjency(path)\n",
    "\n",
    "#5 - estimate the precision of the results\n",
    "with open('m_index_score.pkl','rb') as file:\n",
    "    mynewlist = pickle.load(file)\n",
    "    p_list = mynewlist[0]\n",
    "df = pd.DataFrame(svd_matrix)\n",
    "df.index = p_list\n",
    "df.columns = p_list\n",
    "df.to_pickle('test_score.pkl')\n",
    "dico_rc = benchmark(['test_score.pkl'])\n",
    "plt.plot(dico_rc['test_score.pkl'],label='svd')\n",
    "plt.plot(dico_rc['rand'],label='rand')\n",
    "plt.legend()\n",
    "plt.title('Comparaison svd precision')\n",
    "plt.xlabel('couple de protéine par pas de 1000')\n",
    "plt.ylabel('precision')\n",
    "plt.savefig('Comparaison svd precision.png',dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6453b5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       ...,\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npp(score_matrix)==npp(score_matrix,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20732ba1",
   "metadata": {},
   "source": [
    "# Ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2b95aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test to determine TP,FP,TN and FN in a more appropriate way\n",
    "\n",
    "1 - generate a set of list of every protein couple per distance\n",
    "2 - generate a set of list of every protein couple per KEGG pathway\n",
    "3 - set a threshold which split the first set in two, couple bellow the threshold are\n",
    "gathered as positive, couple above the threshold as negative.\n",
    "4 - intersect the postive set with the KEGG set, the len of the return list correspond to\n",
    "the number of true positive (TP)\n",
    "5 - the len of the positive set minus the number of true positive correspond to the false\n",
    "positive (FP)\n",
    "6 - intersect the negative set with the KEGG set, the len of the return list correspond to\n",
    "the number of false negative (FN)\n",
    "7 - the len of the negative set minus the number of false negative correspond to the true\n",
    "negative (TN)\n",
    "8 - calculate recall and precision\n",
    "9 - change the threshold repeat\n",
    "10 - plot the recall/precision graph\n",
    "'''\n",
    "import itertools as it\n",
    "\n",
    "def get_kegg_couple():\n",
    "    path = 'Data/KEGG/'\n",
    "    list_pathway = 'List_126_KEGG_Path_PA7.txt'\n",
    "    kegg_dico = pathway_dict_builder(list_pathway,path)\n",
    "    kegg_couple = set()\n",
    "    #for each protein in a pathway compute all the possible pair minus double and reverse\n",
    "    for value in kegg_dico.values():\n",
    "        kegg_couple.update(it.combinations(value,2))\n",
    "return kegg_couple\n",
    "\n",
    "def couple_sorter(df):\n",
    "    print('sorting')\n",
    "    couple_dist={}\n",
    "    dico = df.to_dict('split')\n",
    "    for couple in it.combinations(dico['index'],2):\n",
    "        dist=df.loc[couple]\n",
    "        couple_dist.setdefault(dist,[]).append(couple)\n",
    "    couple_dist_ord = dict(sorted(couple_dist.items(),reverse=False))\n",
    "    #print(couple_dist_ord)\n",
    "    print('sorted')\n",
    "    return couple_dist_ord\n",
    "\n",
    "def flat_couple():\n",
    "    positive = list(it.chain.from_iterable(values[:threshold]))\n",
    "    negative = list(it.chain.from_iterable(values[threshold:]))\n",
    "    return positive,negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c126eea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_pathway(list_pathway):\n",
    "    with open(list_pathway) as file:\n",
    "        for l in file:\n",
    "            if l.strip():\n",
    "                buffer = l.split()\n",
    "                yield(buffer[0])\n",
    "                \n",
    "def pathway_dict_builder(list_pathway,path):\n",
    "    dico = {}\n",
    "    list_pathway = f'{path}{list_pathway}'\n",
    "    for pathway in iter_pathway(list_pathway):\n",
    "        file_path = f'{path}{pathway}.txt'\n",
    "        protein = []\n",
    "        with open(file_path) as file:\n",
    "            for l in file:\n",
    "                protein.append(l.strip('\\n'))\n",
    "        dico.setdefault(pathway,[]).extend(protein)  \n",
    "    return dico\n",
    "\n",
    "def get_kegg_couple(pathway_dict):\n",
    "    kegg_dico = pathway_dict\n",
    "    kegg_couple = set()\n",
    "    kegg_positive = set()\n",
    "    #for each protein in a pathway compute all the possible pair minus double and reverse\n",
    "    for value in kegg_dico.values():\n",
    "        kegg_positive.update(it.combinations(value,2))\n",
    "    print(len((kegg_positive)))\n",
    "    #add a way to return all the possible couple\n",
    "    pathway_protein = list(pathway_dict.values())\n",
    "    protein = set(it.chain.from_iterable(pathway_protein))\n",
    "    kegg_couple = set(it.combinations(protein,2))\n",
    "    print(len(kegg_couple))\n",
    "    return kegg_couple,kegg_positive\n",
    "\n",
    "def couple_sorter(df,kegg):\n",
    "    print('sorting')\n",
    "    couple_dist={}\n",
    "    dico = df.to_dict('split')\n",
    "    #for couple in it.combinations(dico['index'],2):\n",
    "    for couple in kegg:\n",
    "        try:\n",
    "            dist=df.loc[couple]\n",
    "            couple_dist.setdefault(dist,[]).append(couple)\n",
    "        except:\n",
    "            continue\n",
    "    couple_dist_ord = dict(sorted(couple_dist.items(),reverse=False))\n",
    "    #print(couple_dist_ord)\n",
    "    print('sorted')\n",
    "    return couple_dist_ord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8138448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Data/KEGG/'\n",
    "list_pathway = 'List_126_KEGG_Path_PA7.txt'\n",
    "df = pd.read_pickle('test_binary_h.pkl')\n",
    "\n",
    "pathway_dict = pathway_dict_builder(list_pathway,path)\n",
    "kegg = get_kegg_couple(pathway_dict)\n",
    "couple = couple_sorter(df,kegg[0])\n",
    "\n",
    "def recall_precision(couple):\n",
    "    values = list(couple.values())\n",
    "    for key in range(len(list(couple.keys()))):\n",
    "        threshold = key\n",
    "\n",
    "        positive = list(it.chain.from_iterable(values[:threshold]))\n",
    "        negative = list(it.chain.from_iterable(values[threshold:]))\n",
    "\n",
    "        TP = len(kegg[1].intersection(positive))\n",
    "        FN = len(kegg[1].intersection(negative))\n",
    "        FP = len(positive) - TP\n",
    "        TN = len(negative) - FN\n",
    "        print(TP,FP)\n",
    "        try:\n",
    "            recall = TP/(TP+FN)\n",
    "            precision = TP/(TP+FP)\n",
    "            print(f'recall:{recall},precision:{precision}')\n",
    "        except:\n",
    "            continue\n",
    "    return recall,precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157b1956",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Data/KEGG/'\n",
    "list_pathway = 'List_126_KEGG_Path_PA7.txt'\n",
    "df = pd.read_pickle('test_binary_h.pkl')\n",
    "\n",
    "kegg = get_kegg_couple(list_pathway,path)\n",
    "couple = couple_sorter(df,kegg)\n",
    "\n",
    "values = list(couple.values())\n",
    "for key in range(len(list(couple.keys()))):\n",
    "    threshold = key\n",
    "    \n",
    "    positive = list(it.chain.from_iterable(values[:threshold]))\n",
    "    negative = list(it.chain.from_iterable(values[threshold:]))\n",
    "\n",
    "    TP = len(kegg.intersection(positive))\n",
    "    FN = len(kegg.intersection(negative))\n",
    "    FP = len(positive) - TP\n",
    "    TN = len(negative) - FN\n",
    "    \n",
    "    try:\n",
    "        recall = TP/(TP+FN)\n",
    "        precision = TP/(TP+FP)\n",
    "        print(f'recall:{recall},precision:{precision}')\n",
    "    except:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
