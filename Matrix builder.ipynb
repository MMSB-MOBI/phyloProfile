{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "836fa0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "#from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07e24f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This script is able to build a coocurence matrix from a list of COG of an organism.\n",
    "It needs to take a csv file containing the list of the COG and the directory where the\n",
    "COG files are stored.\n",
    "It returns a numpy array.\n",
    "'''\n",
    "def iter_query_cog(query_orth):\n",
    "    with open(query_orth) as csvfile:\n",
    "        next(csvfile)\n",
    "        for l in csvfile:\n",
    "            buffer = l.split(';')\n",
    "            yield([buffer[4],buffer[5]])\n",
    "\n",
    "def cog_2_g_list(cog_id,cog_dir):\n",
    "    file_path = f'{cog_dir}/COG_{cog_id}.csv'\n",
    "    g_list = list()\n",
    "    with open(file_path) as csvfile:\n",
    "        next(csvfile)\n",
    "        for l in csvfile:\n",
    "            buffer = l.split(',')\n",
    "            strain = buffer[1]\n",
    "            g_list.append(strain)\n",
    "    return g_list\n",
    "\n",
    "def p_dict_builder(query_orth,cog_dir):\n",
    "    p_dict={}\n",
    "    i=0\n",
    "    for cog_id,p_name in iter_query_cog(query_orth):\n",
    "        g_list = cog_2_g_list(cog_id,cog_dir)\n",
    "        p_dict[p_name] = g_list\n",
    "        #i=i+1\n",
    "        #if i>9:\n",
    "            #break\n",
    "    return p_dict\n",
    "\n",
    "def index_setter(p_dict):\n",
    "    p_index = list()\n",
    "    g_index = set()\n",
    "    for p_name,g_list in p_dict.items():\n",
    "        p_index.append(p_name)\n",
    "        g_index.update(g_list)\n",
    "    g_index = list(g_index)\n",
    "    return p_index,g_index\n",
    "\n",
    "def set_M_values(M,m_index,g_name,p_name,value): \n",
    "    i = m_index[0].index(p_name) #protein index\n",
    "    j = m_index[1].index(g_name) #genome index\n",
    "    M[i,j] = value\n",
    "\n",
    "def matrix_builder(query_orth,cog_dir,value):\n",
    "    #generate a dictionnary of protein coocurences\n",
    "    p_dict=p_dict_builder(query_orth,cog_dir)\n",
    "    with open('p_dict.pkl', 'wb') as file:\n",
    "        pickle.dump(p_dict, file)\n",
    "    #generate the coocurences matrix index\n",
    "    m_index=index_setter(p_dict)\n",
    "    with open('m_index.pkl', 'wb') as file:\n",
    "        pickle.dump(m_index, file)\n",
    "    #generate the coocurences matrix\n",
    "    x = len(m_index[0])\n",
    "    y = len(m_index[1])\n",
    "    M = np.zeros((x,y))\n",
    "    for p_name,g_list in p_dict.items():\n",
    "        for g_name in g_list:\n",
    "            set_M_values(M,m_index,g_name,p_name,value)\n",
    "    with open('p_matrix.npy', 'wb') as f:\n",
    "        np.save(f, M)\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23370fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_p_dict_builder(score_file_dir):\n",
    "    path = score_file_dir\n",
    "    dico_prot={}\n",
    "    for file in os.listdir(path):\n",
    "        dico_strain={}\n",
    "        protein = file.strip('_scores.txt')\n",
    "        file_path = f'{path}{file}'\n",
    "        with open(file_path,'r') as file:\n",
    "            for l in file:\n",
    "                buffer = l.split(',')\n",
    "                strain = buffer[2]\n",
    "                score = buffer[3].strip('\\n')\n",
    "                dico_strain[strain]=score\n",
    "        dico_prot[protein]=dico_strain\n",
    "    return(dico_prot)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3e04e001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_matrix_builder(score_file_dir):\n",
    "    if not os.path.isfile('p_dict_score.pkl'):\n",
    "        p_dict = score_p_dict_builder(score_file_dir)\n",
    "        with open('p_dict_score.pkl', 'wb') as file:\n",
    "            pickle.dump(p_dict, file)\n",
    "    else:\n",
    "        with open('p_dict_score.pkl', 'rb') as file:\n",
    "            p_dict = pickle.load(file)\n",
    "    if not os.path.isfile('m_index_score.pkl'):\n",
    "        m_index = index_setter(p_dict)\n",
    "        with open('m_index_score.pkl', 'wb') as file:\n",
    "            pickle.dump(m_index, file)\n",
    "    else:\n",
    "        with open('m_index_score.pkl', 'rb') as file:\n",
    "            m_index = pickle.load(file)\n",
    "    x = len(m_index[0])\n",
    "    y = len(m_index[1])\n",
    "    M = np.zeros((x,y))\n",
    "    for p_name,g_list in p_dict.items():\n",
    "            for g_name in g_list:\n",
    "                i = m_index[0].index(p_name) #protein index\n",
    "                j = m_index[1].index(g_name) #genome index\n",
    "                M[i,j] = p_dict[p_name][g_name]\n",
    "    with open('p_matrix_score.npy', 'wb') as f:\n",
    "        np.save(f, M)\n",
    "    return(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea762f2",
   "metadata": {},
   "source": [
    "# Binary matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70acca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query_orth ='/Users/mdupuy/Documents/Stage/Pseudomonas_aeruginosa_PA7_119_ortholog_groups.csv'\n",
    "cog_dir='/Users/mdupuy/Documents/Stage/All_COG_groups/'\n",
    "matrix_builder(query_orth,cog_dir,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06114b8b",
   "metadata": {},
   "source": [
    "# Score Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "daa20854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 58s, sys: 1.61 s, total: 2min 59s\n",
      "Wall time: 3min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "score_file_dir = '/Users/mdupuy/Documents/Stage/Parser/Scores/'\n",
    "score_matrix = score_matrix_builder(score_file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77509a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd(M,threshold):\n",
    "    '''\n",
    "    Apply the svd method to a score profile matrix to reduce it noise according to a\n",
    "    certain threshold\n",
    "    '''\n",
    "    M_xmax = np.amax(M,axis=1)\n",
    "    M_lnorm = np.divide(M.T,M_xmax).T\n",
    "    M_lnorm = np.nan_to_num(M_lnorm)\n",
    "    print(\"first normalisation\")\n",
    "    u, s, vh = np.linalg.svd(M_lnorm, full_matrices=False)\n",
    "    s[threshold:]=0.0\n",
    "    P = np.dot(u * s, vh)\n",
    "    print(\"svd\")\n",
    "    P_norm = np.linalg.norm(P,keepdims=True,axis=0)\n",
    "    P_u = np.divide(P,P_norm)\n",
    "    print(\"second normalisation\")\n",
    "    print(\"done\")\n",
    "    return P_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c566dccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 6.67190749e-01 7.16478705e-01 ... 7.03611945e-01\n",
      "  7.17104513e-01 7.85293724e-01]\n",
      " [6.67190749e-01 0.00000000e+00 2.37613320e-03 ... 2.25606867e-03\n",
      "  1.70018793e-03 8.49334288e-03]\n",
      " [7.16478705e-01 2.37613320e-03 0.00000000e+00 ... 1.20317801e-03\n",
      "  7.47370580e-04 3.58443378e-03]\n",
      " ...\n",
      " [7.03611945e-01 2.25606867e-03 1.20317801e-03 ... 0.00000000e+00\n",
      "  8.60653430e-04 4.42645866e-03]\n",
      " [7.17104513e-01 1.70018793e-03 7.47370580e-04 ... 8.60653430e-04\n",
      "  0.00000000e+00 2.88912517e-03]\n",
      " [7.85293724e-01 8.49334288e-03 3.58443378e-03 ... 4.42645866e-03\n",
      "  2.88912517e-03 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "def npp(M):\n",
    "    M=score_matrix\n",
    "    # corrige les valeurs à 0 pour prévenir les artefact\n",
    "    minval = np.min(M[np.nonzero(M)])\n",
    "    score_matrix[score_matrix==0]=minval\n",
    "    # normalisation par la taille\n",
    "    M_xmax = np.amax(M,axis=1)\n",
    "    M_lnorm = np.divide(M.T,M_xmax).T\n",
    "    # transformation monotonique\n",
    "    M_transf = np.reciprocal(M_lnorm)\n",
    "    # z-score\n",
    "    M_ymean = np.mean(M_transf,axis=0,keepdims=True)\n",
    "    M_ystd = np.std(M_transf,ddof=0,axis=0,keepdims=True)\n",
    "    M = np.divide(np.subtract(M,M_ymean),M_ystd)\n",
    "    return M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6b3d96",
   "metadata": {},
   "source": [
    "# Distance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20b5bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "from scipy.spatial.distance import squareform\n",
    "def hamming(coocurencies_matrix):\n",
    "    hamming = pdist(coocurencies_matrix, metric='hamming')\n",
    "    distance_matrix = squareform(hamming)\n",
    "    with open('hd_matrix.pkl', 'wb') as f:\n",
    "        pickle.dump(distance_matrix,f)\n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cb8e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_M = hamming(M)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
